{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2955d66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 2 to Frame 4:\n",
      "  00:00:00:02\n",
      "  00:00:00:03\n",
      "  00:00:00:04\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 31 to Frame 33:\n",
      "  00:00:01:07\n",
      "  00:00:01:08\n",
      "  00:00:01:09\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 67 to Frame 70:\n",
      "  00:00:02:19\n",
      "  00:00:02:20\n",
      "  00:00:02:21\n",
      "  00:00:02:22\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 122 to Frame 123:\n",
      "  00:00:05:02\n",
      "  00:00:05:03\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 155: 00:00:06:11\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1023: 00:00:42:15\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1111 to Frame 1112:\n",
      "  00:00:46:07\n",
      "  00:00:46:08\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1160: 00:00:48:08\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1201 to Frame 1205:\n",
      "  00:00:50:01\n",
      "  00:00:50:02\n",
      "  00:00:50:03\n",
      "  00:00:50:04\n",
      "  00:00:50:05\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1211 to Frame 1215:\n",
      "  00:00:50:11\n",
      "  00:00:50:12\n",
      "  00:00:50:13\n",
      "  00:00:50:14\n",
      "  00:00:50:15\n",
      "\n",
      "Location: /hpsans12/production/Dune2/reel1/VFX/Hydraulx\n",
      "Valid Frames to fix:\n",
      "Frame 1251 to Frame 1253:\n",
      "  00:00:52:03\n",
      "  00:00:52:04\n",
      "  00:00:52:05\n",
      "\n",
      "Location: /hpsans12/production/Dune2/reel1/VFX/Hydraulx\n",
      "Valid Frames to fix:\n",
      "Frame 1260: 00:00:52:12\n",
      "\n",
      "Location: /hpsans12/production/Dune2/reel1/VFX/Hydraulx\n",
      "Valid Frames to fix:\n",
      "Frame 1270 to Frame 1272:\n",
      "  00:00:52:22\n",
      "  00:00:52:23\n",
      "  00:00:53:00\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1302 to Frame 1303:\n",
      "  00:00:54:06\n",
      "  00:00:54:07\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1310: 00:00:54:14\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1500: 00:01:02:12\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5000 to Frame 5002:\n",
      "  00:03:28:08\n",
      "  00:03:28:09\n",
      "  00:03:28:10\n",
      "\n",
      "Location: /hpsans15/production/Dune2/pickups/shot_1ab/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5010 to Frame 5014:\n",
      "  00:03:28:18\n",
      "  00:03:28:19\n",
      "  00:03:28:20\n",
      "  00:03:28:21\n",
      "  00:03:28:22\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5111: 00:03:32:23\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5122: 00:03:33:10\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5133: 00:03:33:21\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5144: 00:03:34:08\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5155: 00:03:34:19\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5166: 00:03:35:06\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 2 to Frame 4:\n",
      "  00:00:00:02\n",
      "  00:00:00:03\n",
      "  00:00:00:04\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 31 to Frame 33:\n",
      "  00:00:01:07\n",
      "  00:00:01:08\n",
      "  00:00:01:09\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 67 to Frame 70:\n",
      "  00:00:02:19\n",
      "  00:00:02:20\n",
      "  00:00:02:21\n",
      "  00:00:02:22\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 122 to Frame 123:\n",
      "  00:00:05:02\n",
      "  00:00:05:03\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 155: 00:00:06:11\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1023: 00:00:42:15\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1111 to Frame 1112:\n",
      "  00:00:46:07\n",
      "  00:00:46:08\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1160: 00:00:48:08\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1201 to Frame 1205:\n",
      "  00:00:50:01\n",
      "  00:00:50:02\n",
      "  00:00:50:03\n",
      "  00:00:50:04\n",
      "  00:00:50:05\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1211 to Frame 1215:\n",
      "  00:00:50:11\n",
      "  00:00:50:12\n",
      "  00:00:50:13\n",
      "  00:00:50:14\n",
      "  00:00:50:15\n",
      "\n",
      "Location: /hpsans12/production/Dune2/reel1/VFX/Hydraulx\n",
      "Valid Frames to fix:\n",
      "Frame 1251 to Frame 1253:\n",
      "  00:00:52:03\n",
      "  00:00:52:04\n",
      "  00:00:52:05\n",
      "\n",
      "Location: /hpsans12/production/Dune2/reel1/VFX/Hydraulx\n",
      "Valid Frames to fix:\n",
      "Frame 1260: 00:00:52:12\n",
      "\n",
      "Location: /hpsans12/production/Dune2/reel1/VFX/Hydraulx\n",
      "Valid Frames to fix:\n",
      "Frame 1270 to Frame 1272:\n",
      "  00:00:52:22\n",
      "  00:00:52:23\n",
      "  00:00:53:00\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1302 to Frame 1303:\n",
      "  00:00:54:06\n",
      "  00:00:54:07\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1310: 00:00:54:14\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 1500: 00:01:02:12\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5000 to Frame 5002:\n",
      "  00:03:28:08\n",
      "  00:03:28:09\n",
      "  00:03:28:10\n",
      "\n",
      "Location: /hpsans15/production/Dune2/pickups/shot_1ab/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5010 to Frame 5014:\n",
      "  00:03:28:18\n",
      "  00:03:28:19\n",
      "  00:03:28:20\n",
      "  00:03:28:21\n",
      "  00:03:28:22\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5111: 00:03:32:23\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5122: 00:03:33:10\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5133: 00:03:33:21\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5144: 00:03:34:08\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5155: 00:03:34:19\n",
      "\n",
      "Location: /hpsans13/production/Dune2/reel1/partA/1920x1080\n",
      "Valid Frames to fix:\n",
      "Frame 5166: 00:03:35:06\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "3 columns passed, passed data had 2 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Users\\stuff\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:969\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Users\\stuff\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:1017\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m-> 1017\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_mi_list:\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 3 columns passed, passed data had 2 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 221\u001b[0m\n\u001b[0;32m    218\u001b[0m     populate_xytech_collection(xytech_file)\n\u001b[0;32m    220\u001b[0m valid_frames \u001b[38;5;241m=\u001b[39m process_collectionsVideo(video_file_path)\n\u001b[1;32m--> 221\u001b[0m export_to_xls(valid_frames, output_file_path)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Process collections and check frames against video duration\u001b[39;00m\n\u001b[0;32m    223\u001b[0m process_collectionsVideo(video_file_path)\n",
      "Cell \u001b[1;32mIn[9], line 184\u001b[0m, in \u001b[0;36mexport_to_xls\u001b[1;34m(data, output_file)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport_to_xls\u001b[39m(data, output_file):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Create DataFrame\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimecode\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# Save DataFrame to Excel file\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_excel(output_file, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Users\\stuff\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:746\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 746\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    747\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    748\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[0;32m    749\u001b[0m         data,\n\u001b[0;32m    750\u001b[0m         columns,\n\u001b[0;32m    751\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    752\u001b[0m         dtype,\n\u001b[0;32m    753\u001b[0m     )\n\u001b[0;32m    754\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    755\u001b[0m         arrays,\n\u001b[0;32m    756\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    759\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    760\u001b[0m     )\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Users\\stuff\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:510\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 510\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    511\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Users\\stuff\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:875\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    872\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    873\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 875\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32mD:\\Users\\stuff\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:972\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    969\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 972\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m    975\u001b[0m     contents \u001b[38;5;241m=\u001b[39m _convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 3 columns passed, passed data had 2 columns"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pymongo\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "# MongoDB connection information\n",
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = mongo_client[\"your_database_name\"]\n",
    "\n",
    "# Function to populate Baselight collection\n",
    "def populate_baselight_collection(baselight_file):\n",
    "    baselight_collection = db[\"baselight\"]\n",
    "\n",
    "    output_data = []\n",
    "\n",
    "    for line in baselight_file:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "        parts = line.split()\n",
    "        folder = parts[0]\n",
    "        frames = [int(frame) for frame in parts[1:] if frame.isdigit()]\n",
    "        output_data.append({\"folder\": folder, \"frames\": frames})\n",
    "\n",
    "    # Insert data into Baselight collection\n",
    "    baselight_collection.insert_many(output_data)\n",
    "\n",
    "def populate_xytech_collection(xytech_file):\n",
    "    xytech_collection = db[\"xytech\"]\n",
    "    output_data = []\n",
    "\n",
    "    workorder = None\n",
    "    location = []\n",
    "    notes = []\n",
    "\n",
    "    for line in xytech_file:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "        if line.startswith(\"Xytech Workorder\"):\n",
    "            if workorder is not None:  # Save previous data if any\n",
    "                output_data.append({\"workorder\": workorder, \"location\": location, \"notes\": \"\\n\".join(notes)})\n",
    "                location = []  # Reset location for next entry\n",
    "                notes = []  # Reset notes for next entry\n",
    "            workorder = line.split()[-1]  # Extract workorder number\n",
    "        elif line.startswith(\"Location:\"):\n",
    "            continue  # Skip Location: line\n",
    "        elif line.startswith(\"Notes:\"):\n",
    "            continue  # Skip Notes: line\n",
    "        else:\n",
    "            # Assume it's a location path or a note\n",
    "            if line.startswith(\"/\"):  # Check if it's a location path\n",
    "                location.append(line)  # Add to location list\n",
    "            else:\n",
    "                notes.append(line)  # Add to notes list\n",
    "\n",
    "    # Save the last entry after loop ends\n",
    "    if workorder is not None:\n",
    "        output_data.append({\"workorder\": workorder, \"location\": location, \"notes\": \"\\n\".join(notes)})\n",
    "\n",
    "    # Insert data into Xytech collection\n",
    "    xytech_collection.insert_many(output_data)\n",
    "\n",
    "\n",
    "#proj1 script\n",
    "def process_collections():\n",
    "    # Query the database for Baselight collections\n",
    "    baselight_collection = db[\"baselight\"]\n",
    "    cursor = baselight_collection.find({}, {\"_id\": 0, \"folder\": 1, \"frames\": 1})\n",
    "\n",
    "    output_ranges = []\n",
    "\n",
    "    # Process each document in the Baselight collection\n",
    "    for document in cursor:\n",
    "        currentFolder = document[\"folder\"]\n",
    "        parseline = [str(frame) for frame in document[\"frames\"]]  # Convert frames to strings\n",
    "\n",
    "        parseFolder = currentFolder.split(\"/\")  # Split current folder by \"/\"\n",
    "        if len(parseFolder) > 1:\n",
    "            parseFolder.pop(1)  # Remove the second element if exists\n",
    "        newFolder = \"/\".join(parseFolder)  # Reconstruct the folder path\n",
    "\n",
    "        for techfile in XY_File:\n",
    "            if newFolder in techfile:\n",
    "                currentFolder = techfile.strip()\n",
    "\n",
    "        tempStart = None\n",
    "        tempLast = None\n",
    "        for number in parseline:\n",
    "            if not number.isdigit():\n",
    "                continue\n",
    "            number = int(number)\n",
    "            if tempStart is None:\n",
    "                tempStart = number\n",
    "                tempLast = number\n",
    "            elif number == tempLast + 1:\n",
    "                tempLast = number\n",
    "            else:\n",
    "                if tempStart == tempLast:\n",
    "                    output_ranges.append((currentFolder, tempStart))\n",
    "                else:\n",
    "                    output_ranges.append((currentFolder, f\"{tempStart}-{tempLast}\"))\n",
    "                tempStart = number\n",
    "                tempLast = number\n",
    "        if tempStart is not None:\n",
    "            if tempStart == tempLast:\n",
    "                output_ranges.append((currentFolder, tempStart))\n",
    "            else:\n",
    "                output_ranges.append((currentFolder, f\"{tempStart}-{tempLast}\"))\n",
    "\n",
    "    return output_ranges\n",
    "\n",
    "# Function to get video duration using ffmpeg\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "def get_video_duration(video_file):\n",
    "    command = ['ffprobe', '-v', 'error', '-select_streams', 'v:0', '-show_entries', 'stream=duration,r_frame_rate', '-of', 'csv=p=0', video_file]\n",
    "    try:\n",
    "        output = subprocess.check_output(command, stderr=subprocess.STDOUT).decode('utf-8').strip()\n",
    "        parts = output.split(',')\n",
    "        if len(parts) > 1:\n",
    "            duration_str = parts[0]\n",
    "            if '/' in duration_str:\n",
    "                numerator, denominator = map(float, duration_str.split('/'))\n",
    "                duration = numerator / denominator\n",
    "            else:\n",
    "                duration = float(duration_str)\n",
    "            frame_rate_str = parts[1]\n",
    "            if '/' in frame_rate_str:\n",
    "                numerator, denominator = map(float, frame_rate_str.split('/'))\n",
    "                frame_rate = numerator / denominator\n",
    "            else:\n",
    "                frame_rate = float(frame_rate_str)\n",
    "        else:\n",
    "            duration = float(parts[0])\n",
    "            frame_rate = 24  # Assuming default frame rate as 24 fps\n",
    "        video_duration = duration * frame_rate\n",
    "        return video_duration\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: Failed to get video duration for {video_file}.\")\n",
    "        print(f\"Command: {' '.join(command)}\")\n",
    "        print(f\"Error message: {e.output.decode('utf-8').strip()}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to process collections and filter frames based on video duration\n",
    "def process_collectionsVideo(video_file):\n",
    "    # Get video duration\n",
    "    video_duration = get_video_duration(video_file)\n",
    "\n",
    "    # Get valid frames from collections\n",
    "    valid_ranges = process_collections()\n",
    "\n",
    "    # Filter frames based on video duration\n",
    "    valid_frames = []\n",
    "    for folder, frames in valid_ranges:\n",
    "        if isinstance(frames, int):\n",
    "            if frames < video_duration:\n",
    "                valid_frames.append((folder, frames))\n",
    "        else:\n",
    "            start_frame, end_frame = map(int, frames.split('-'))\n",
    "            if start_frame < video_duration:\n",
    "                valid_frames.append((folder, frames))\n",
    "\n",
    "    # Print valid frames with timecodes for current folder\n",
    "    for folder, frames in valid_frames:\n",
    "        print(f\"Location: {folder}\")\n",
    "        print(\"Valid Frames to fix:\")\n",
    "        if isinstance(frames, int):\n",
    "            v\n",
    "        else:\n",
    "            start_frame, end_frame = map(int, frames.split('-'))\n",
    "            print(f\"Frame {start_frame} to Frame {end_frame}:\")\n",
    "            for frame in range(start_frame, end_frame + 1):\n",
    "                print(f\"  {frame_to_timecode(frame)}\")\n",
    "        print()         \n",
    "    return valid_frames\n",
    "       \n",
    "\n",
    "    \n",
    "def export_to_xls(data, output_file):\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"Location\", \"Frames\", \"Timecode\"])\n",
    "\n",
    "    # Save DataFrame to Excel file\n",
    "    df.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"Data exported to {output_file}\")\n",
    "\n",
    "# Function to convert frame number to timecode\n",
    "def frame_to_timecode(frame):\n",
    "    fps = 24  # Assuming 24 frames per second\n",
    "    total_seconds = frame / fps\n",
    "    hours = int(total_seconds // 3600)\n",
    "    minutes = int((total_seconds % 3600) // 60)\n",
    "    seconds = int(total_seconds % 60)\n",
    "    frames = int(frame % fps)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}:{frames:02d}\"\n",
    "            \n",
    "# Paths to Baselight and Xytech files\n",
    "baselight_file_path = \"Baselight_export.txt\"\n",
    "xytech_file_path = \"Xytech.txt\"\n",
    "video_file_path = \"twitch_nft_demo.mp4\"\n",
    "output_file_path = \"output.xlsx\"\n",
    "\n",
    "BL_File = open(\"Baselight_export.txt\", \"r\")  # Open Baselight file\n",
    "#Xytech\n",
    "with open(\"Xytech.txt\") as f:\n",
    "    XY_File = f.read().splitlines()\n",
    "\n",
    "# Open Baselight file\n",
    "with open(baselight_file_path, \"r\") as baselight_file:\n",
    "    populate_baselight_collection(baselight_file)\n",
    "\n",
    "# Open Xytech file\n",
    "with open(xytech_file_path, \"r\") as xytech_file:\n",
    "    populate_xytech_collection(xytech_file)\n",
    "    \n",
    "valid_frames = process_collectionsVideo(video_file_path)\n",
    "export_to_xls(valid_frames, output_file_path)\n",
    "# Process collections and check frames against video duration\n",
    "process_collectionsVideo(video_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb9b82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty line, skipping...\n",
      "CSV file exported successfully!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "BL_File = open(\"Baselight_export.txt\", \"r\")  # Open Baselight file\n",
    "#Xytech\n",
    "with open(\"Xytech.txt\") as f:\n",
    "    XY_File = f.read().splitlines()\n",
    "isVerbose = 0  # Set to True for verbose output\n",
    "\n",
    "output_data = []  # List to store data for CSV export\n",
    "\n",
    "for currentReadLine in BL_File:\n",
    "    parseline = currentReadLine.split()  # Split the line\n",
    "    if parseline:  # Check if the list is not empty\n",
    "        currentFolder = parseline.pop(0)  # Get the current folder\n",
    "        parseFolder = currentFolder.split(\"/\")  # Split current folder by \"/\"\n",
    "        if len(parseFolder) > 1:\n",
    "            parseFolder.pop(1)  # Remove the second element if exists\n",
    "        newFolder = \"/\".join(parseFolder)  # Reconstruct the folder path\n",
    "        for techfile in XY_File:\n",
    "            if newFolder in techfile:\n",
    "                currentFolder = techfile.strip()\n",
    "        tempStart = None\n",
    "        tempLast = None\n",
    "        for number in parseline:\n",
    "            if not number.isdigit():\n",
    "                continue\n",
    "            number = int(number)\n",
    "            if tempStart is None:\n",
    "                tempStart = number\n",
    "                tempLast = number\n",
    "            elif number == tempLast + 1:\n",
    "                tempLast = number\n",
    "            else:\n",
    "                if tempStart == tempLast:\n",
    "                    output_data.append([currentFolder, tempStart])\n",
    "                else:\n",
    "                    output_data.append([currentFolder, f\"{tempStart}-{tempLast}\"])  # Concatenate range\n",
    "                tempStart = number\n",
    "                tempLast = number\n",
    "        if tempStart is not None:\n",
    "            if tempStart == tempLast:\n",
    "                output_data.append([currentFolder, tempStart])\n",
    "            else:\n",
    "                output_data.append([currentFolder, f\"{tempStart}-{tempLast}\"])  # Concatenate range\n",
    "    else:\n",
    "        print(\"Empty line, skipping...\")\n",
    "\n",
    "BL_File.close()  # Close Baselight file\n",
    "\n",
    "# Write data to CSV file\n",
    "with open(\"output.csv\", \"w\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"Producer\", \"Operator\", \"Job\" , \"Notes\"])  # Write header\n",
    "    for _ in range(2):  # Skip 4 rows\n",
    "        writer.writerow([])\n",
    "    writer.writerow([\"Location\", \"frames to fix\"])\n",
    "    for data in output_data:\n",
    "        writer.writerow(data)  # Write data\n",
    "\n",
    "print(\"CSV file exported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e35c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
